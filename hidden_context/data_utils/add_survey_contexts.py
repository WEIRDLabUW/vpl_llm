# This file is used to preprocess dataset, available for any HH-RLHF format datasets
import os
from dataclasses import dataclass, field
from typing import Optional, cast
from tqdm import tqdm
import random

from transformers import (
    HfArgumentParser,
    AutoTokenizer,
    AutoModelForSequenceClassification,
    AutoModelForCausalLM,
    AutoConfig,
    AutoModel
)

import torch

from hidden_context.train_llm_preference_model import (
    DataSubset,
    get_hh_rlhf_dataset,
    concatenate_datasets,
    HHRLHFPreprocessor,
)
from datasets import load_dataset

from copy import deepcopy

import numpy as np

import sys, ipdb, traceback


def info(type, value, tb):
    traceback.print_exception(type, value, tb)
    ipdb.pm()


sys.excepthook = info


@dataclass
class ScriptArguments:
    output_dir: Optional[str] = field(
        metadata={"help": "Directory where the new dataset will be stored."},
    )
    data_path: str = field(
        metadata={"help": "Directory where the original data is stored."}
    )
    data_subset: str = field(
        default="helpful",
        metadata={
            "help": "Which subset of the data to use. You can choose between"
                    "'helpful', or 'harmless'."
        },
    )
    data_split: str = field(
        default="test",
        metadata={
            "help": "Which split of the data to use. You can choose between"
                    "'train', or 'test'."
        },
    )
    dataset_size: int = field(
        default=0,
        metadata={"help": "The size of the subset of the data to use"},
    )
    model_type: str = field(
        default="none",
        metadata={
            "help": "You can choose between 'gpt2', 'llama', or 'none'."
        }
    )
    embed_dim: int = field(
        default=1024,
        metadata={
            "help": "Dimension of the embeddings generated by LLM."
        }
    )
    max_length: int = field(
        default=1024,
        metadata={
            "help": "Maximum token length of the inputs."
        }
    )
    with_embeddings: bool = field(
        default=True,
        metadata={
            "help": "Whether the embeddings are generated during pre-processing."
        }
    )
    add_controversial: bool = field(
        default=False,
        metadata={
            "help": "Whether to add an extra feature which indicates whether the preference is controversial."
        }
    )
    synthetic_dataset: bool = field(
        default=False,
        metadata={
            "help": "Whether a synthetic dataset is used."
        }
    )
    use_causal_lm: bool = field(default=False)
    other_subsets: str = field(default=None)
    survey_size: int = field(default=8)
    context_length: int = field(default=8)
    controversial_only: bool = field(default=True)
    num_duplicates: int = field(default=1)
    fixed_context_length: bool = field(default=False)
    random_contexts: bool = field(default=False)


def generate_embeddings_with_llm(args, input_dataset=None):
    """
    This function is used to generate fixed embeddings for inputs from original dataset.
    """
    if not args.synthetic_dataset:
        data_subset = cast(DataSubset, args.data_subset)
        input_dataset = get_hh_rlhf_dataset(
            data_subset,
            args.data_split,
            args.dataset_size,
            data_path=args.data_path,
            use_subset_as_dir=True,
            other_subsets=args.other_subsets,
        )
    if args.model_type == "gpt2":
        tokenizer = AutoTokenizer.from_pretrained("gpt2", use_auth_token=True)
        if not args.use_causal_lm:
            model = AutoModelForSequenceClassification.from_pretrained(
                "gpt2", num_labels=args.embed_dim, torch_dtype=torch.bfloat16
            )
            model.score.weight.data *= 0.01
        else:
            model = AutoModelForCausalLM.from_pretrained(
                "gpt2", torch_dtype=torch.bfloat16
            )
    elif args.model_type == "llama" or args.model_type == "meta-llama/Llama-2-7b-hf":
        tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf", use_auth_token=True, add_eos_token=False)
        if not args.use_causal_lm:
            model = AutoModelForSequenceClassification.from_pretrained(
                "meta-llama/Llama-2-7b-hf", num_labels=args.embed_dim, torch_dtype=torch.bfloat16
            )
        else:
            model = AutoModelForCausalLM.from_pretrained(
                "meta-llama/Llama-2-7b-hf", torch_dtype=torch.bfloat16
            )
    elif args.model_type == "llm2vec":
        from llm2vec import LLM2Vec
        from peft import PeftModel

        tokenizer = AutoTokenizer.from_pretrained(
            "McGill-NLP/LLM2Vec-Mistral-7B-Instruct-v2-mntp"
        )
        config = AutoConfig.from_pretrained(
            "McGill-NLP/LLM2Vec-Mistral-7B-Instruct-v2-mntp", trust_remote_code=True
        )
        model = AutoModel.from_pretrained(
            "McGill-NLP/LLM2Vec-Mistral-7B-Instruct-v2-mntp",
            trust_remote_code=True,
            config=config,
            torch_dtype=torch.bfloat16,
            device_map="cuda" if torch.cuda.is_available() else "cpu",
        )
        model = PeftModel.from_pretrained(
            model,
            "McGill-NLP/LLM2Vec-Mistral-7B-Instruct-v2-mntp",
        )
        model = model.merge_and_unload()  # This can take several minutes on cpu
        model = LLM2Vec(model, tokenizer, pooling_mode="mean", max_length=4096)
    else:
        return input_dataset
    model.to("cuda")
    
    if not args.model_type == "llm2vec":
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id
        model.config.pad_token_id = tokenizer.pad_token_id
        tokenizer.padding_side = "right"

        dataset_size = len(input_dataset)
        print(dataset_size)

        preprocessed_dataset = input_dataset.map(
            HHRLHFPreprocessor(tokenizer),
            batched=True,
            num_proc=24,
            remove_columns=input_dataset.column_names,
            load_from_cache_file=False,
        )

        input_dataset = input_dataset.filter(
            lambda example, idx: len(preprocessed_dataset[idx]["input_ids_chosen"]) <= args.max_length
                                and len(preprocessed_dataset[idx]["input_ids_rejected"]) <= args.max_length,
            with_indices=True
        )
        preprocessed_dataset = preprocessed_dataset.filter(
            lambda example: len(example["input_ids_chosen"]) <= args.max_length
                            and len(example["input_ids_rejected"]) <= args.max_length
        )
    else:
        dataset_size = len(input_dataset)
        preprocessed_dataset = input_dataset
    print(len(input_dataset), len(preprocessed_dataset))
    dataset_size = len(preprocessed_dataset)

    embeddings = list()
    batch_size = 256
    prompts = []
    if args.model_type == "llm2vec":
        dataset_size += 1
    for row_id in tqdm(range(dataset_size)):
        if args.model_type == "llm2vec":
            if (row_id % batch_size == 0 or row_id == dataset_size - 1) and row_id > 0:
                embs = model.encode(prompts)
                # import pdb; pdb.set_trace()
                embs = torch.nn.functional.normalize(embs, p=2, dim=1)

                # import pdb; pdb.set_trace()
                for batch_id in range(0, embs.shape[0], 2):
                    # temp = torch.nn.functional.normalize(temp, p=2, dim=1)
                    chosen_emb = embs[batch_id]
                    rejected_emb = embs[batch_id + 1]
                    emb = dict()
                    emb[f"embedding_chosen"] = chosen_emb.float().cpu().numpy()
                    emb[f"embedding_rejected"] = rejected_emb.float().cpu().numpy()
                    embeddings.append(emb)
                prompts = []
            if row_id == dataset_size - 1:
                break
            chosen_prompt = preprocessed_dataset[row_id]["chosen"]
            rejected_prompt = preprocessed_dataset[row_id]["rejected"]
            instruction = "Given a prompt and response reason if the response is truthful, honest, instruction following or helpful. "

            prompts.append([instruction, chosen_prompt])
            prompts.append([instruction, rejected_prompt])
        else:
            emb = dict()
            for key in ['chosen', 'rejected']:
                tokens = tokenizer.pad(
                    {"input_ids": preprocessed_dataset[row_id][f"input_ids_{key}"]},
                    padding=True, pad_to_multiple_of=64, return_tensors="pt"
                )
                token_length = len(preprocessed_dataset[row_id][f"input_ids_{key}"])
                input_ids = tokens["input_ids"].unsqueeze(0).to("cuda")
                attention_mask = tokens["attention_mask"].unsqueeze(0).to("cuda")
                with torch.no_grad():
                    if not args.use_causal_lm:
                        # emb[f"embedding_{key}"] = model(
                        #     input_ids=input_ids,
                        #     attention_mask=attention_mask
                        # )[0][0].float().cpu().numpy()
                        # import ipdb; ipdb.set_trace()
                        last_hidden_state = model(
                            input_ids=input_ids,
                            attention_mask=attention_mask,
                            output_hidden_states=True
                        ).hidden_states[-1]
                        emb[f"embedding_{key}"] = last_hidden_state[0][token_length - 1].float().cpu().numpy()
                    else:
                        last_hidden_state = model(
                            input_ids=input_ids,
                            attention_mask=attention_mask,
                            output_hidden_states=True
                        ).hidden_states[-1]
                        masked_last_hidden_state = last_hidden_state * attention_mask.unsqueeze(-1)
                        token_length = torch.sum(attention_mask, dim=1)
                        mean_pooling = torch.sum(masked_last_hidden_state, dim=1) / token_length
                        emb[f"embedding_{key}"] = mean_pooling[0].float().cpu().numpy()
            embeddings.append(emb)
    output_dataset = input_dataset.add_column("embeddings", embeddings)
    return output_dataset


def generate_contexts(args, input_dataset, survey_dataset):
    output_dir = os.path.join(args.output_dir, f"{args.model_type}", f"{args.data_subset}")
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    import ipdb; ipdb.set_trace()
    if args.controversial_only:
        input_dataset = input_dataset.filter(lambda x: x['controversial'] == True)
    dataset_size = len(input_dataset)
    if args.data_split == 'train':
        K = args.num_duplicates  # repeat samples for K times
    else:
        K = 1
    dataset_list = list()

    def random_choice(max_context_length, survey_size):
        if max_context_length <= survey_size:
            from functools import reduce
            while True:
                if args.fixed_context_length:
                    context_length = max_context_length
                else:
                    if args.other_subsets == '84':
                        context_length = random.randint(1, max_context_length)
                    else:
                        context_length = random.randint(2, max_context_length)
                context_chosen_ids = np.random.choice(survey_size, context_length, replace=False)
                chosen_dataset = [d for idx, d in enumerate(survey_dataset) if idx in context_chosen_ids]
                if args.other_subsets != 'single':
                    return chosen_dataset, context_length
                satisfied_sets = list()
                for row in chosen_dataset:
                    satisfied_sets.append(set(row["satisfied_subset"]))
                if len(reduce(lambda x, y: x.intersection(y), satisfied_sets)) == 1:
                    return chosen_dataset, context_length
                elif context_length == survey_size:
                    raise ValueError("Please choose another random seed!")
        else:
            raise ValueError("Context length is larger than survey size!")

    for idx in range(K):
        output_dataset = deepcopy(input_dataset)
        context_lengths = list()
        contexts = list()
        for _ in tqdm(range(dataset_size)):  # iterate over all samples in original dataset
            row_contexts = list()

            context_dataset, context_length = random_choice(args.context_length, args.survey_size)
            context_lengths.append(context_length)
            for context_row in context_dataset:
                context_id = context_row["Index"]
                context_data = context_row
                if not args.with_embeddings:
                    row_contexts.append({
                        'original_id': context_id,
                        'chosen': context_data['chosen'],
                        'rejected': context_data['rejected'],
                    })
                else:
                    row_contexts.append({
                        'original_id': context_id,
                        'embedding_chosen': context_data['embeddings']['embedding_chosen'],
                        'embedding_rejected': context_data['embeddings']['embedding_rejected'],
                    })
            contexts.append(row_contexts)
        output_dataset = output_dataset.add_column("context_length", context_lengths)
        output_dataset = output_dataset.add_column("contexts", contexts)
        output_dataset.map()
        dataset_list.append(output_dataset)
    output = concatenate_datasets(dataset_list)
    output.to_json(os.path.join(output_dir, f"{args.data_split}.jsonl"))
    return output


if __name__ == "__main__":
    # default setting on HH-RLHF dataset, please iterate over data subsets and data splits
    seed = 0
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    parser = HfArgumentParser(ScriptArguments)
    script_args: ScriptArguments = parser.parse_args_into_dataclasses()[0]
    print(script_args)
    dataset = generate_embeddings_with_llm(script_args)
    if not script_args.random_contexts:
        survey_options = dataset.filter(lambda x: x['survey_options'] == True)
    else:
        survey_options = dataset.filter(lambda x: x['survey_options'] == True or x['survey_options'] == False)
    survey_ids = np.random.choice(range(len(survey_options)), script_args.survey_size, replace=False)
    print(survey_ids)
    if script_args.data_split == "train":
        survey_data = survey_options.filter(lambda example, idx: idx in survey_ids, with_indices=True)
        survey_data.to_json(os.path.join(script_args.data_path, script_args.data_subset, "survey_{}.jsonl".format(script_args.survey_size)))
    else:
        survey_data = load_dataset('json' , data_files=os.path.join(script_args.data_path, script_args.data_subset, "survey_{}.jsonl".format(script_args.survey_size)))
        survey_data = survey_data['train']
    generate_contexts(script_args, dataset, survey_data)

# python -m hidden_context.data_utils.data_processing --output_dir data/relabeled_hh_rlhf_in_context_fixed/
# --data_path data/relabeled_hh_rlhf --data_subset helpful --data_split train --model_type gpt2
